{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dependancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import inflect\n",
    "import pandas as pd\n",
    "import uuid\n",
    "from moviepy.editor import *\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import detect_nonsilent\n",
    "import wave\n",
    "import scipy.io.wavfile\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from IPython.display import Audio\n",
    "import seaborn as sns\n",
    "from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Creating video frames from .mp4 files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data i.e TikTok emoji challenge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All videos are saved here!\n",
    "main_directory = \"E:\\\\tiktok\"\n",
    "\n",
    "skip_files = [\"raw\",\n",
    "              \"ready\",\n",
    "              \"try to make a face that fits my description\",\n",
    "              \"face challenge\",\n",
    "              \"clown face\"]\n",
    "\n",
    "\n",
    "all_emo_files=[main_directory+\"\\\\\"+ i for i in os.listdir(main_directory) if i not in skip_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The follownig faces are prompted in the TikTok emoji challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['E:\\\\tiktok\\\\angry face',\n",
       " 'E:\\\\tiktok\\\\crying face',\n",
       " 'E:\\\\tiktok\\\\disappointed face',\n",
       " 'E:\\\\tiktok\\\\Extracted Faces',\n",
       " 'E:\\\\tiktok\\\\grinning face with clenched teeth',\n",
       " 'E:\\\\tiktok\\\\grinning face with normal eyes',\n",
       " 'E:\\\\tiktok\\\\nauseated face',\n",
       " 'E:\\\\tiktok\\\\slightly smiling face',\n",
       " 'E:\\\\tiktok\\\\thinking face',\n",
       " 'E:\\\\tiktok\\\\winking face']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_emo_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting video frames and creating a seperate directory for each emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path_frames = os.path.join(main_directory,\"Extracted Faces\"); os.mkdir(path_frames)\n",
    "\n",
    "for emo in all_emo_files:\n",
    "    print(\"Reading ->\",emo)\n",
    "    list_of_vids = []\n",
    "    \n",
    "    for paths in glob.glob(os.path.join(emo, \"*mp4\")):\n",
    "        list_of_vids.append(paths)\n",
    "    path_frames_emotion = os.path.join(path_frames,emo[len(main_directory)+1:])\n",
    "    os.mkdir(path_frames_emotion)\n",
    "    \n",
    "    for v in tqdm.tqdm(list_of_vids):\n",
    "        vidcap = cv2.VideoCapture(v)\n",
    "        success,image = vidcap.read()\n",
    "        count = 0\n",
    "        \n",
    "        while success:\n",
    "            cv2.imwrite(path_frames_emotion+\"\\\\\"+\"{}_frame_{}.jpg\".format(v[len(emo)+1:][:-4],count+1), image)\n",
    "            success,image = vidcap.read()\n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Detecting and scaling facial expressions + Eliminating dim lit frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating a sub-directory\n",
    "faces_normalized = os.path.join(\"E:\\\\tiktok\\\\Extracted Faces\",\"faces normalized\")\n",
    "os.mkdir(faces_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_all_faces = \"E:\\\\tiktok\\\\Extracted Faces\"\n",
    "\n",
    "for i in tqdm.tqdm(range(len(os.listdir(path_all_faces)))):\n",
    "    expression = os.path.join(faces_normalized,os.listdir(path_all_faces)[i])\n",
    "    os.mkdir(expression)\n",
    "        # Initialize face counter\n",
    "    face_count = 1\n",
    "    list_of_imgs = os.path.join(path_all_faces,os.listdir(path_all_faces)[i])\n",
    "    \n",
    "    for img in (glob.glob(os.path.join(list_of_imgs,\"*jpg\"))):\n",
    "\n",
    "        # Read image using cv2.imread()\n",
    "        image = cv2.imread(img)\n",
    "\n",
    "        # Construct HAAR cascade \n",
    "        faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "        # Locate all face coordinates\n",
    "        faces = faceCascade.detectMultiScale(image, scaleFactor=1.5, minNeighbors=10, minSize=(50, 50))\n",
    "\n",
    "        # If you have rotated images\n",
    "        timeout = 3  # Set a timer to exit an infinite while loop if it occurs\n",
    "        \n",
    "        while (len(faces)==0 and timeout!=0):\n",
    "            # Iteratively rotate images clockwise to detect faces\n",
    "            image = cv2.rotate(image,cv2.ROTATE_90_CLOCKWISE)\n",
    "\n",
    "            # Construct HAAR cascade\n",
    "            faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "            # Locate all face coordinates\n",
    "            faces = faceCascade.detectMultiScale(image, scaleFactor=2.5, minNeighbors=11, minSize=(50, 50))\n",
    "\n",
    "            # Countdown\n",
    "            timeout-=1\n",
    "\n",
    "        # Read faces \n",
    "        for x,y,h,w in faces:\n",
    "            cv2.imwrite(expression+\"\\\\\"+\"{}_face {}.jpg\".format(img[len(list_of_imgs)+1:][:-4],\n",
    "                                                                face_count),image[y:y+h, x:x+w])\n",
    "            face_count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
